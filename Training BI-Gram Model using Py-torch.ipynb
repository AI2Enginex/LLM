{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ7QCLkW6os3"
      },
      "source": [
        "# **Generating Vocabulary using Gutenberg project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9uMnWjC5wMB",
        "outputId": "082d24b2-3333-42af-ed4e-304a514e48ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ubTXrhZr8Cqf"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y5HevUPA7LVf"
      },
      "outputs": [],
      "source": [
        "def vocabulary(filename):\n",
        "  try:\n",
        "    with open(filename, 'r', encoding='unicode_escape') as f:\n",
        "      text = f.read()\n",
        "    return sorted(set(text))\n",
        "  except Exception as e:\n",
        "    return e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Q87VRF7MAv"
      },
      "source": [
        "# **Encoding and Decoding Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7sF8nHkqkXp",
        "outputId": "a32edd87-f5a9-4817-90dd-8bac590f1835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[61, 58, 65, 65, 68, 1, 61, 68, 76, 1, 54, 71, 58, 1, 78, 68, 74] hello how are you\n"
          ]
        }
      ],
      "source": [
        "class Transformers:\n",
        "\n",
        "\n",
        "  def __init__(self,filename):\n",
        "\n",
        "    self.vocabulary = vocabulary(filename)\n",
        "\n",
        "  def encode(self,input_str):\n",
        "    try:\n",
        "      string_dict = { ch:i for i,ch in enumerate(self.vocabulary) }\n",
        "      return [string_dict[c] for c in input_str]\n",
        "    except Exception as e:\n",
        "      return e\n",
        "\n",
        "  def decode(self,input_lst):\n",
        "    try:\n",
        "      int_to_string = { i:ch for i,ch in enumerate(self.vocabulary) }\n",
        "      return ''.join([int_to_string[i] for i in input_lst])\n",
        "    except Exception as e:\n",
        "      return e\n",
        "\n",
        "t = Transformers(filename='/content/drive/MyDrive/wizard_of_oz.txt')\n",
        "encoded_text = t.encode('hello how are you')\n",
        "decoded_text = t.decode(encoded_text)\n",
        "\n",
        "\n",
        "print(encoded_text , decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBGahiZz8SVf"
      },
      "source": [
        "# **Encoding using PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0TF5VMPtq_0M"
      },
      "outputs": [],
      "source": [
        "encoded_str  = torch.tensor(t.encode(\"hello hello, can you hear us?\"), dtype=torch.long)\n",
        "decode_str = t.decode(encoded_str.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEEE6Sr7-pcp",
        "outputId": "1908383d-8eb4-454d-d77a-8bbd1d0ee94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([61, 58, 65, 65, 68,  1, 61, 58, 65, 65, 68,  9,  1, 56, 54, 67,  1, 78,\n",
            "        68, 74,  1, 61, 58, 54, 71,  1, 74, 72, 24])\n"
          ]
        }
      ],
      "source": [
        "print(encoded_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaCZZneI-rYr",
        "outputId": "b501c24f-ef6c-49b8-b689-aaaf737814b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello hello, can you hear us?\n"
          ]
        }
      ],
      "source": [
        "print(decode_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzxSPVrAr5C"
      },
      "source": [
        "# **Training using BiGrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l_4HjzgPDuQy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_file(filename):\n",
        "\n",
        "  try:\n",
        "    with open(filename, 'r',encoding='unicode_escape') as f:\n",
        "      text = f.read()\n",
        "    return text\n",
        "  except Exception as e:\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z46XFQ-A_27W"
      },
      "outputs": [],
      "source": [
        "data_input = read_file(filename='/content/drive/MyDrive/wizard_of_oz.txt')\n",
        "\n",
        "data_for_training = torch.tensor(t.encode(data_input), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2BB1PtkGAzi",
        "outputId": "bf413562-7707-4c60-91a1-7d87c7e0569d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "batch_size = 4\n",
        "max_iters = 1000\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lliwd6SBEl9",
        "outputId": "0dee9908-7d08-4835-d52a-b3e322d80f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[65, 58, 57,  1, 73, 68, 67, 58],\n",
            "        [62, 71, 72,  1, 62, 67,  1, 73],\n",
            "        [61,  1, 62, 73, 11,  1, 44, 61],\n",
            "        [58, 71,  5, 72,  1, 63, 54, 62]], device='cuda:0')\n",
            "targets:\n",
            "tensor([[58, 57,  1, 73, 68, 67, 58, 11],\n",
            "        [71, 72,  1, 62, 67,  1, 73, 61],\n",
            "        [ 1, 62, 73, 11,  1, 44, 61, 58],\n",
            "        [71,  5, 72,  1, 63, 54, 62, 65]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate_training_set(data_lst):\n",
        "\n",
        "  try:\n",
        "     split_percentage = int(0.80 * len(data_lst))\n",
        "     return data_lst[:split_percentage] , data_lst[split_percentage:]\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    return e\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "    train_data , val_data = generate_training_set(data_lst = data_for_training)\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "print(x.shape)\n",
        "print(x)\n",
        "print('targets:')\n",
        "print(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QVtL_l7vHqLO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bigram Model**"
      ],
      "metadata": {
        "id": "rkcOxNjs8h2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lTnhWHLz5PI",
        "outputId": "5f4073bd-a950-4abc-915f-8ee59637ee7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 5.051, val loss: 5.035\n",
            "step: 250, train loss: 4.960, val loss: 4.942\n",
            "step: 500, train loss: 4.898, val loss: 4.889\n",
            "step: 750, train loss: 4.821, val loss: 4.822\n",
            "4.9785847663879395\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        logits = self.token_embedding_table(index)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            logits, loss = self.forward(index)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            index = torch.cat((index, index_next), dim=1)\n",
        "        return index\n",
        "\n",
        "class Optimizer:\n",
        "\n",
        "  def __init__(self, file_name):\n",
        "\n",
        "    vocab_size = len(vocabulary(filename=file_name))\n",
        "    self.model = BigramLanguageModel(vocab_size)\n",
        "    m = self.model.to(device)\n",
        "    self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(self):\n",
        "    out = {}\n",
        "    self.model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = self.model(X, Y)   # dependent and independent variables\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    self.model.train()\n",
        "    return out\n",
        "  def train_model(self):\n",
        "    for iter in range(max_iters):\n",
        "      if iter % eval_iters == 0:\n",
        "        losses = self.estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb = get_batch('train')\n",
        "\n",
        "      # evaluate the loss\n",
        "      logits, loss = self.model.forward(xb, yb)\n",
        "      self.optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "    print(loss.item())\n",
        "\n",
        " # moving model to GPU\n",
        "\n",
        "o = Optimizer(file_name='/content/drive/MyDrive/wizard_of_oz.txt')\n",
        "o.train_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}